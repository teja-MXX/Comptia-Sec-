# ðŸ“˜ FEYNMAN NOTES â€” Obfuscation: Steganography, Tokenization, Data Masking

Obfuscation = techniques used to hide information, not necessarily encrypt it.  
Goal = make sensitive data unnoticeable, unusable, or unreadable without authorized systems.

The transcript focuses on three major obfuscation techniques:

1. Steganography (hide data inside other data)
    
2. Tokenization (replace sensitive data with meaningless stand-ins)
    
3. Data Masking (modify data so itâ€™s still usable but not sensitive)
    

---

# 1. STEGANOGRAPHY (Technical Breakdown)

### Definition

Steganography = hiding data inside another file or message in a way that conceals the existence of the hidden data.

- Unlike encryption â†’ which signals â€œthere is secret dataâ€,
    
- Steganography tries to make the data invisible.
    

The hidden message is embedded by manipulating parts of the file that humans cannot detect (e.g., low-order pixel bits).

### Formal Description

- Data is embedded into a carrier file (image, audio, text, video).
    
- Techniques often modify Least Significant Bits (LSB) of pixels or samples.
    
- The carrier appears unchanged to the human observer.
    

### Purpose

- Avoid suspicion.
    
- Hide communication.
    
- Often combined with encryption â†’ encrypted data is then hidden inside a file.
    

### Key Property

- Not encryption.  
    If someone knows the embedding method or uses a hex editor, they can extract the data.
    

### Transcript Demonstration (Technical Notes)

A tool was used to:

1. Select an image (`logo.png`).
    
2. Input a secret message.
    
3. Encode â†’ embed the binary of the message into the image pixel data.
    
4. Save the stego-image.
    

Observations:

- Visual appearance stays the same â†’ human eye canâ€™t detect LSB-level changes.
    
- File size changed (74 KB â†’ 57 KB).  
    This happens due to:
    
    - Re-encoding
        
    - Compression differences
        

This is a detection indicator:

> identical images with different sizes may indicate hidden data.

### Decoding

- Using the same tool:
    
    - Original image â†’ no output message (because no hidden data)
        
    - Modified image â†’ hidden text extracted perfectly
        

### Important Technical Note

- The hidden message remains visible in:
    
    - Raw bytes (hex editor)
        
    - Metadata analysis
        
    - Specialized steganalysis tools
        
- Steganography â‰  cryptography.
    

---

# 2. TOKENIZATION (Technical Breakdown)

### Definition

Tokenization = replacing sensitive data with a non-sensitive placeholder (â€œtokenâ€) that:

- Has no mathematical or exploitable relationship to the original value.
    
- Has zero intrinsic meaning.
    
- Can only be reverse-mapped by a secure tokenization system.
    

### How It Works

1. Sensitive value enters system (e.g., credit card number).
    
2. A random or pseudorandom token is generated.
    
3. Original value is stored in a secure token vault.
    
4. Token is returned and used by systems that do not need the actual data.
    
5. De-tokenization only happens on authorized systems.
    

### Key Features

- Tokens cannot be reverse engineered.
    
- Tokens can be format-preserving (e.g., same length, same character type).
    
- Tokens are useless to attackers.
    

### Use Case (From Transcript)

Retailer storing card numbers:

- Merchant systems store tokens, not real card numbers.
    
- If a breach happens â†’ attacker gets meaningless tokens.
    
- Real card data is stored separately in a high-security vault.
    

### Regulatory Use

Tokenization is essential for:

- PCI-DSS compliance
    
- Payment systems
    
- High-risk industries
    

---

# 3. DATA MASKING (Technical Breakdown)

### Definition

Data masking = modifying real data so it remains:

- Structurally valid
    
- Semantically realistic
    
- Operationally useful for testing or analytics
    
- But NOT sensitive
    

It protects real identities or values.

### Purpose

Used where:

- Teams need realistic test data
    
- But cannot use actual customer data
    
- And cannot expose sensitive fields internally
    

### Types of Masking

- Static masking: produce a sanitized copy of a database
    
- Dynamic masking: mask data in real time during retrieval
    
- Deterministic masking: same input â†’ same masked output
    
- Randomized masking: unpredictable outputs
    

### Examples From Transcript

1. Development environments
    
    - Replace real customer names with realistic fake names.
        
    - Modify addresses slightly so they are invalid but plausible.
        
2. Internal views for employees
    
    - Credit card numbers show only last 4 digits.  
        First 12 digits masked â†’ reduces insider threat.
        
1. Medical offices
    
    - Use only partial SSN for identity verification.
        
    - Full value never shown to employees.
        

### Key Property

Masked data:

- Remains useful
    
- But cannot reconstruct original value
    

This makes masking ideal for testing, analytics, support, and internal workflows.

---

# 4. FINAL SUMMARY â€” The Three Techniques (Feynman-style)

> Steganography hides data inside other data to conceal that anything is hidden.  
> Tokenization replaces a sensitive value with a meaningless random value; only a secure vault can map it back.  
> Data Masking modifies data so it looks real and works for testing but doesnâ€™t expose sensitive content.

All three are forms of obfuscation, not necessarily encryption.

### Differences in One Line Each

- Steganography: Hide the existence of data.
    
- Tokenization: Replace data with a non-sensitive surrogate.
    
- Data Masking: Transform data so it is usable but not sensitive.
    

### Why They Matter

They reduce:

âœ” attack surface  
âœ” internal exposure  
âœ” breach impact  
âœ” compliance burden